---
title: "记一次生产服务器debug(上)"
date: 2020-04-14T10:27:22+11:00
draft: true
summary: 1. 17:18分，nginx的error日志里开始出现upstream 110 connection timeout的记录。2. 17:22分， netdata开始报警20min steal cpu = 18.2%， 并很快上升31.8%。 3. 17:27分， 群里开始有人报告相应服务不能访问。 4. 17:30分， 远程登录服务器后发现cpu使用率非常的高（主要是node server在占用），基本在80-100使用率，但是服务器的访问量并不高。
---

线索：

**开始修复前：**

1. 17:18分，nginx的error日志里开始出现upstream 110 connection timeout的记录。
2. 17:22分， netdata开始报警20min steal cpu = 18.2%， 并很快上升31.8%
3. 17:27分， 群里开始有人报告相应服务不能访问
4. 17:30分， 远程登录服务器后发现cpu使用率非常的高（主要是node server在占用），基本在80-100使用率，但是服务器的访问量并不高。


**开始修复：**

1. 重启production的node server，并关闭对应的test服务。问题依旧存在。单纯应用出错的可能性基本排除。
2. 因为之前发生过admin输入错误的tag导致应用不能正常运行的情况，而且根据nginx的记录是17：18才开始突然出现，怀疑db里的数据出了问题。于是import了staging的db的数据，但是问题依旧存在。排除db数据出错的原因。
3. 因为短时间找不到问题的根源，怀疑问题可能出现在比较底层的地方。于是决定先迁移到新的服务器，使服务先可以访问再说。迁移过去后访问恢复正常。但是即使在很低访问量的情况下**cpu的使用率依旧很高**。

**恢复访问后的debug:**

1. 在旧的服务器上对staging server进行压力测试，1秒10+个request，持续半个小时，使cpu保持在80-100%之间。但是无法重现服务无法访问的情况, steal cpu基本正常。为查找根源带来了极大阻碍。
2. 对nginx的error log进一步分析，发现错误大部分是在等node server response的时候超时（2000+次），还有一部分是（400+）是和node server建立connection的时候出错（应该是node server在debug的时候关掉过几次导致），socket无法建立而导致无法访问的可能性较小。目前看是建立TCP连接后node服务器响应超时的缘故。结合很高的steal cpu，可以推断node server出了问题。

**第一阶段分析的结论和疑点：**

根据目前分析，大概率是node server在处理请求的时候超时导致的缘故。但是有几点比较奇怪的地方：

   1. 为什么node server的cpu使用率这么高？整个请求路径是怎么样的？中间每一步的耗时情况，cpu的使用率是怎么样的？
   2. 如果只是单纯node server出了问题，为什么重启之后，问题依旧存在？而迁移到另一个服务器后问题却没了？ 怀疑还有帮凶，而且出现在系统底层。因为目前无法重现无法访问的情况，给这方面的debug带来了很大的困难。

**一些措施：**

1. 研究strapi的框架，对cms的代码进行code review。在此基础上对服务进行profile搞清楚cpu使用率高的原因。
2. 对strapi进行docker化，这样之后再出问题了，可以将部署时间极大减少。
3. 如有可能将strapi进行拆分，将admin端和api端分开（这块不熟悉，还要看完代码才能决定）。将api端放上去kubernetes，避免单点故障。
