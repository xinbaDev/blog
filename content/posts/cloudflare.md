---
Title: "Some Thoughts on the Cloudflare Incident Happened a Few Days Ago"
Date: 2022-06-25T10:49:00+11:00
Summary: A few days ago, there was a major incident with Cloudflare's network. Since many important websites depend on Cloudflare's network, many websites were unable to access and even Discord was temporarily unavailable. Later, I saw on Twitter that it was a problem with Cloudflare. The next day, Cloudflare released a detailed incident report on their official blog, meeting my expectations for the company's transparency and analysis. Here, I mainly want to record my thoughts on this matter.
draft: false
---

As a DevOps professional, I am particularly concerned about Cloudflare's incident. Based on my observation of the company, I expected them to quickly release a detailed report, which they did. Cloudflare is a leader in its industry and has many excellent programmers and network engineers. Any modifications to their network have a set of release mechanisms. According to the article, changes to network settings require a request ticket, multiple rounds of review, a dry run, a small subnet release, and a step-by-step release in other networks. Despite this, the incident still occurred. This incident was not simple, and according to the blog post, it occurred in the latter part of the rollout, when Cloudflare introduced a new architecture that did not affect the previous architecture. This shows that the introduction of new technology can increase the cost of operations and the possibility of errors. Cloudflare is a highly innovative company, and such problems are inevitable. It reminds me of the long periods when Twitter and Facebook were unable to function correctly, which demonstrates the difficulty of maintaining a website/api.

Aside from the incident's cause, I am also concerned about Cloudflare's response strategy and how to avoid a recurrence. As a DevOps professional, this is a critical issue that I must consider when facing operational incidents. Cloudflare has proposed three solutions, starting with program updates and architecture considerations, and then implementing automation measures. From a program perspective, they updated their old release policy and introduced a new architecture. From an architectural perspective, they eliminated the possibility of incorrect sorting and technically avoided the same problem from reoccurring. From an automation perspective, they introduced corresponding commit-confirm rollback mechanisms to reduce time to resolve. This incident's timeline was also attached, even including the engineers reverting changes, which was quite transparent. 

To be honest, such incidents, especially in a company like Cloudflare with a strong architectural design, are usually not a good sign. An incident is often the result of many issues adding up, similar to an airplane's design, which typically does not have an accident. However, when an accident occurs, there are usually many underlying problems. As a critical infrastructure provider on the Internet, Cloudflare also has similar problems, which is worrying. However, it is worth noting that Cloudflare was very transparent about this incident, providing detailed analysis of the incident's cause and solution to prevent a recurrence, which is very sincere. I am quite satisfied with Cloudflare's response. In a rapidly evolving company, the introduction of many new technological architectures often increases the possibility of errors. This is understandable, but my expectations for Cloudflare are high. This incident revealed some shortcomings in Cloudflare's operations. I recall that Netflix developed an open-source software called "Chaos Monkey" to randomly shut down virtual machines to test their system's reliability. I believe that this mindset is necessary for every DevOps professional -- Can the reliability of a component still be guaranteed if it has a problem? If it can't, how can the impact be minimized?